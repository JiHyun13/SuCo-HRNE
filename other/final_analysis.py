import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# í´ë” ê²½ë¡œ ì„¤ì •
scores_dir = "output/results/"
distribution_dir = "output/distributions/"
performance_dir = "output/performance_curves/"
output_csv_path = "output/threshold_analysis_summary.csv"
os.makedirs(distribution_dir, exist_ok=True)
os.makedirs(performance_dir, exist_ok=True)

# ê¸°ì¤€ threshold (ì‹œê°í™” ë° ê¸°ì¤€ ë¹„êµìš©)
threshold = 50.0  # 0.0 ~ 100.0 ë²”ìœ„ë¡œ ì„¤ì •

# ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
results = []
all_best_thresholds = []

# ğŸ“Š ë¶„í¬ ì‹œê°í™” í•¨ìˆ˜
def save_distribution_plot(query_score, reference_scores, filename, threshold):
    plt.figure(figsize=(10,6))
    sns.histplot(reference_scores, bins=50, kde=True, label="Reference")
    plt.axvline(query_score, color='red', linestyle='--', label=f"Query: {query_score:.2f}")
    plt.axvline(threshold, color='green', linestyle='-', label=f"Threshold: {threshold:.2f}")
    plt.xlabel("Similarity Score")
    plt.ylabel("Count")
    plt.title(f"Similarity Score Distribution\n{filename}")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    save_path = os.path.join(distribution_dir, f"distribution_{filename.replace('.csv', '.png')}")
    plt.savefig(save_path)
    plt.close()

# ğŸ“ˆ ì„±ëŠ¥ ê³¡ì„  ì‹œê°í™” í•¨ìˆ˜
def save_performance_curve_plot(y_true, y_scores, filename):
    def evaluate(t):
        y_pred = [1 if s >= t else 0 for s in y_scores]
        return (
            accuracy_score(y_true, y_pred),
            precision_score(y_true, y_pred, zero_division=0),
            recall_score(y_true, y_pred, zero_division=0),
            f1_score(y_true, y_pred, zero_division=0)
        )

    thresholds = [i for i in range(10, 101,1)]
    accs, precs, recs, f1s = zip(*[evaluate(t) for t in thresholds])

    plt.figure(figsize=(10,6))
    plt.plot(thresholds, accs, label="Accuracy")
    plt.plot(thresholds, precs, label="Precision")
    plt.plot(thresholds, recs, label="Recall")
    plt.plot(thresholds, f1s, label="F1 Score")
    plt.xlabel("Threshold")
    plt.ylabel("Score")
    plt.title(f"Performance by Threshold\n{filename}")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    save_path = os.path.join(performance_dir, f"performance_{filename.replace('.csv', '.png')}")
    plt.savefig(save_path)
    plt.close()

# ğŸ” íŒŒì¼ ë°˜ë³µ ë¶„ì„
file_list = [f for f in os.listdir(scores_dir) if f.startswith("scores_eucli_") and f.endswith(".csv")]
total_files = len(file_list)

for i, filename in enumerate(file_list):
    print(f"ğŸ” [{i+1}/{total_files}] ë¶„ì„ ì¤‘: {filename}")

    filepath = os.path.join(scores_dir, filename)
    df = pd.read_csv(filepath)

    try:
        query_row = df[df["type"] == "query"].iloc[0]
        query_score = query_row["similarity"]
        query_id = str(query_row["song_id"]).zfill(5)
        reference_df = df[df["type"] == "reference"]
    except IndexError:
        print(f"âš ï¸ ë°ì´í„° ì´ìƒìœ¼ë¡œ ê±´ë„ˆëœ€: {filename}")
        continue

    # ì •ë‹µ ê¸°ì¤€: reference filenameì— query_idê°€ í¬í•¨ë˜ë©´ ì •ë‹µ
    y_true = reference_df["filename"].apply(lambda x: 1 if query_id in x else 0).tolist()
    y_scores = reference_df["similarity"].tolist()

    y_true = [1] + y_true  # queryëŠ” í•­ìƒ ì •ë‹µ
    y_scores = [query_score] + y_scores

    # âœ… ìµœì  threshold íƒìƒ‰
    best_f1 = -1
    best_threshold = None
    for t in [i for i in range(10, 101,1)]:  # 0.0 ~ 100.0
        y_pred_tmp = [1 if s >= t else 0 for s in y_scores]
        f1_tmp = f1_score(y_true, y_pred_tmp, zero_division=0)
        if f1_tmp > best_f1:
            best_f1 = f1_tmp
            best_threshold = t

    print(f"   â†³ ìµœì  threshold: {best_threshold:.1f}, Best F1: {best_f1:.4f}")
    all_best_thresholds.append(best_threshold)

    # ê¸°ì¤€ thresholdë¡œ í‰ê°€
    y_pred = [1 if s >= best_threshold else 0 for s in y_scores]
    acc = accuracy_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred, zero_division=0)
    rec = recall_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)

    results.append({
        "Filename": filename,
        "Accuracy": round(acc, 4),
        "Precision": round(prec, 4),
        "Recall": round(rec, 4),
        "F1_Score": round(f1, 4),
        "Query_Score": round(query_score, 4),
        "Best_Threshold": round(best_threshold, 2),
        "Best_F1_Score": round(best_f1, 4)
    })

    # ê·¸ë˜í”„ ì €ì¥
    save_distribution_plot(query_score, reference_df["similarity"].tolist(), filename,best_threshold)
    save_performance_curve_plot(y_true, y_scores, filename)
    print(f"   ğŸ“ˆ ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {filename.replace('.csv', '')}")

# CSV ì €ì¥
summary_df = pd.DataFrame(results)
summary_df.to_csv(output_csv_path, index=False)

# ì¶”ì²œ threshold ê³„ì‚°
recommended_threshold = round(pd.Series(all_best_thresholds).mean(), 2)

# ë¡œê·¸ ì¶œë ¥
print(f"ğŸ“Š ì „ì²´ {len(results)}ê°œ íŒŒì¼ ë¶„ì„ ì™„ë£Œ")
print(f"ğŸ“ˆ í‰ê·  ì •í™•ë„: {summary_df['Accuracy'].mean():.4f}")
print(f"âœ… ì¶”ì²œ ì„ê³„ê°’ (ì „ì²´ í‰ê· ): {recommended_threshold}")
print(f"ğŸ“ ë¶„ì„ ìš”ì•½ CSV ì €ì¥ ì™„ë£Œ: {output_csv_path}")
